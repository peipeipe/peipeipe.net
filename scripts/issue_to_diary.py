#!/usr/bin/env python3
"""
GitHub Issues ã‚’æ—¥è¨˜ã‚¨ãƒ³ãƒˆãƒªã«å¤‰æ›ã™ã‚‹ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
"""

import os
import yaml
import re
import requests
import base64
import io
from datetime import datetime, timezone, timedelta
from pathlib import Path
from urllib.parse import urlparse
from PIL import Image
import io

def escape_html(text):
    """HTMLã‚¨ã‚¹ã‚±ãƒ¼ãƒ—"""
    text = text.replace('&', '&amp;')
    text = text.replace('<', '&lt;')
    text = text.replace('>', '&gt;')
    text = text.replace('"', '&quot;')
    text = text.replace("'", '&#x27;')
    return text

def get_jst_now():
    """æ—¥æœ¬æ™‚é–“ã®ç¾åœ¨æ™‚åˆ»ã‚’å–å¾—"""
    jst = timezone(timedelta(hours=9))
    return datetime.now(jst)

def convert_to_webp(image_data, quality=85, max_width=1200):
    """ç”»åƒãƒ‡ãƒ¼ã‚¿ã‚’WebPã«å¤‰æ›ï¼ˆãƒªã‚µã‚¤ã‚ºãƒ»Exifå‰Šé™¤æ©Ÿèƒ½ä»˜ãï¼‰"""
    try:
        # ãƒã‚¤ãƒŠãƒªãƒ‡ãƒ¼ã‚¿ã‹ã‚‰PIL Imageã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ä½œæˆ
        image = Image.open(io.BytesIO(image_data))
        original_size = image.size
        
        # Exifãƒ‡ãƒ¼ã‚¿ã‚’ç¢ºèªãƒ»å‰Šé™¤
        exif_info = []
        if hasattr(image, '_getexif') and image._getexif() is not None:
            exif_dict = image._getexif()
            
            # ã‚ˆãå«ã¾ã‚Œã‚‹Exifæƒ…å ±ã‚’ãƒã‚§ãƒƒã‚¯
            dangerous_tags = {
                34853: 'GPSæƒ…å ±',  # GPSInfo
                306: 'æ’®å½±æ—¥æ™‚',    # DateTime
                271: 'ã‚«ãƒ¡ãƒ©ãƒ¡ãƒ¼ã‚«ãƒ¼',  # Make
                272: 'ã‚«ãƒ¡ãƒ©ãƒ¢ãƒ‡ãƒ«',   # Model
                37500: 'ãƒ¡ãƒ¼ã‚«ãƒ¼ãƒãƒ¼ãƒˆ'  # MakerNote
            }
            
            for tag_id, description in dangerous_tags.items():
                if tag_id in exif_dict:
                    exif_info.append(description)
            
            if exif_info:
                print(f"âš ï¸  æ¤œå‡ºã•ã‚ŒãŸExifãƒ‡ãƒ¼ã‚¿: {', '.join(exif_info)}")
                print("ğŸ”’ ã“ã‚Œã‚‰ã®æƒ…å ±ã¯å®Œå…¨ã«å‰Šé™¤ã•ã‚Œã¾ã™")
            else:
                print("âœ… å±é™ºãªExifãƒ‡ãƒ¼ã‚¿ãªã—")
        else:
            print("âœ… Exifãƒ‡ãƒ¼ã‚¿ãªã—")
        
        # ç”»åƒã‚’ãƒªã‚µã‚¤ã‚ºï¼ˆå¹…ãŒæœ€å¤§å¹…ã‚’è¶…ãˆã‚‹å ´åˆï¼‰
        if image.width > max_width:
            # ã‚¢ã‚¹ãƒšã‚¯ãƒˆæ¯”ã‚’ä¿æŒã—ã¦ãƒªã‚µã‚¤ã‚º
            ratio = max_width / image.width
            new_height = int(image.height * ratio)
            image = image.resize((max_width, new_height), Image.Resampling.LANCZOS)
            print(f"ç”»åƒãƒªã‚µã‚¤ã‚º: {original_size[0]}x{original_size[1]} -> {image.width}x{image.height}")
        
        # RGBAãƒ¢ãƒ¼ãƒ‰ã®å‡¦ç†ï¼ˆé€æ˜åº¦å¯¾å¿œï¼‰
        if image.mode in ('RGBA', 'LA', 'P'):
            # é€æ˜åº¦ãŒã‚ã‚‹å ´åˆã¯ãã®ã¾ã¾ä¿æŒ
            pass
        elif image.mode != 'RGB':
            image = image.convert('RGB')
        
        # WebPå½¢å¼ã§ãƒã‚¤ãƒŠãƒªãƒ‡ãƒ¼ã‚¿ã«å¤‰æ›ï¼ˆExifãƒ‡ãƒ¼ã‚¿ã¯è‡ªå‹•çš„ã«å‰Šé™¤ã•ã‚Œã‚‹ï¼‰
        webp_buffer = io.BytesIO()
        image.save(webp_buffer, format='WEBP', quality=quality, optimize=True)
        webp_data = webp_buffer.getvalue()
        
        # å…ƒã®ã‚µã‚¤ã‚ºã¨å¤‰æ›å¾Œã®ã‚µã‚¤ã‚ºã‚’è¡¨ç¤º
        original_file_size = len(image_data)
        webp_file_size = len(webp_data)
        compression_ratio = (1 - webp_file_size / original_file_size) * 100
        
        print(f"WebPå¤‰æ›: {original_file_size:,} bytes -> {webp_file_size:,} bytes (-{compression_ratio:.1f}%)")
        print("ğŸ”’ ãƒ—ãƒ©ã‚¤ãƒã‚·ãƒ¼ä¿è­·: Exif/GPSãƒ‡ãƒ¼ã‚¿ã¯å®Œå…¨ã«å‰Šé™¤ã•ã‚Œã¾ã—ãŸ")
        
        return webp_data
        
    except Exception as e:
        print(f"WebPå¤‰æ›ã‚¨ãƒ©ãƒ¼: {e}")
        return image_data  # å¤‰æ›ã«å¤±æ•—ã—ãŸå ´åˆã¯å…ƒã®ãƒ‡ãƒ¼ã‚¿ã‚’è¿”ã™

def download_and_save_image(image_url, date_str, time_str):
    """GitHubç”»åƒURLã‹ã‚‰ç”»åƒã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¦ä¿å­˜"""
    try:
        print(f"ç”»åƒãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰é–‹å§‹: {image_url}")
        
        # ç”»åƒã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
        response = requests.get(image_url, timeout=30)
        response.raise_for_status()
        
        # å…ƒã®æ‹¡å¼µå­ã‚’å–å¾—ï¼ˆãƒ­ã‚°ç”¨ï¼‰
        parsed_url = urlparse(image_url)
        path_parts = parsed_url.path.split('.')
        original_ext = 'unknown'
        if len(path_parts) > 1:
            original_ext = path_parts[-1].lower()
        
        # WebPã«å¤‰æ›
        webp_data = convert_to_webp(response.content)
        
        # ä¿å­˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆï¼ˆæ—¥ä»˜åˆ¥ãƒ•ã‚©ãƒ«ãƒ€ï¼‰
        image_dir = Path(f'images/{date_str}')
        image_dir.mkdir(parents=True, exist_ok=True)
        
        # ãƒ•ã‚¡ã‚¤ãƒ«åã‚’ç”Ÿæˆï¼ˆWebPå½¢å¼ã§ä¿å­˜ï¼‰
        timestamp = time_str.replace(':', '')
        filename = f"{timestamp}.webp"
        image_path = image_dir / filename
        
        # ç”»åƒã‚’ä¿å­˜
        with open(image_path, 'wb') as f:
            f.write(webp_data)
        
        print(f"ç”»åƒã‚’ä¿å­˜ã—ã¾ã—ãŸ ({original_ext} -> webp): {image_path}")
        return f"/images/{date_str}/{filename}"
        
    except Exception as e:
        print(f"ç”»åƒä¿å­˜ã‚¨ãƒ©ãƒ¼ ({image_url}): {e}")
        return None

def process_images_in_content(content, date_str, time_str):
    """GitHubã®ç”»åƒURLã‚’è¦‹ã¤ã‘ã¦ã€ãƒ­ãƒ¼ã‚«ãƒ«ã«ä¿å­˜ã—ã€ãƒ‘ã‚¹ã‚’æ›´æ–°"""
    print("ç”»åƒURLã®æ¤œç´¢ã¨å‡¦ç†ã‚’é–‹å§‹...")
    print(f"æ¤œç´¢å¯¾è±¡ã®å†…å®¹: {content[:500]}...")  # ãƒ‡ãƒãƒƒã‚°ç”¨
    
    # GitHubç”»åƒURLã®ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆuser-attachmentsã€camoã€raw.githubusercontent.com ãªã©ï¼‰
    github_image_patterns = [
        r'!\[[^\]]*\]\(https://github\.com/user-attachments/assets/[^)]+\)',
        r'!\[[^\]]*\]\(https://user-images\.githubusercontent\.com/[^)]+\)',
        r'!\[[^\]]*\]\(https://camo\.githubusercontent\.com/[^)]+\)',
        r'!\[[^\]]*\]\(https://raw\.githubusercontent\.com/[^)]+\.(png|jpg|jpeg|gif|webp)\)',
        r'!\[[^\]]*\]\(https://github-production-user-asset-[^)]+\.s3\.amazonaws\.com/[^)]+\)',
        r'!\[[^\]]*\]\(https://private-user-images\.githubusercontent\.com/[^)]+\)',
        r'https://github\.com/user-attachments/assets/[^)\s]+',
        r'https://user-images\.githubusercontent\.com/[^)\s]+',
        r'https://camo\.githubusercontent\.com/[^)\s]+',
        r'https://raw\.githubusercontent\.com/[^)\s]+\.(png|jpg|jpeg|gif|webp)',
        r'https://github-production-user-asset-[^)\s]+\.s3\.amazonaws\.com/[^)\s]+'
    ]
    
    updated_content = content
    image_counter = 1
    
    for pattern in github_image_patterns:
        print(f"ãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œç´¢ä¸­: {pattern}")  # ãƒ‡ãƒãƒƒã‚°ç”¨
        matches = re.finditer(pattern, updated_content, re.IGNORECASE)
        for match in matches:
            image_url = match.group(0)
            print(f"GitHubç”»åƒURLç™ºè¦‹: {image_url}")
            
            # Markdownãƒªãƒ³ã‚¯ã®å ´åˆã¯URLã‚’æŠ½å‡º
            if image_url.startswith('!['):
                # ![...](URL) ã‹ã‚‰URLã‚’æŠ½å‡º
                url_match = re.search(r'\(([^)]+)\)', image_url)
                if url_match:
                    actual_url = url_match.group(1)
                    print(f"Markdownã‹ã‚‰æŠ½å‡ºã—ãŸURL: {actual_url}")
                else:
                    continue
            else:
                actual_url = image_url
            
            # ã‚¯ã‚¨ãƒªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’é™¤å»
            clean_url = actual_url.split('?')[0].split('#')[0]
            
            # ç”»åƒã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¦ä¿å­˜
            local_path = download_and_save_image(clean_url, date_str, f"{time_str}-{image_counter:02d}")
            
            if local_path:
                # å…ƒã®ãƒãƒƒãƒã—ãŸæ–‡å­—åˆ—ã‚’ç½®æ›
                if image_url.startswith('!['):
                    # Markdownãƒªãƒ³ã‚¯ã®å ´åˆ
                    updated_content = updated_content.replace(image_url, f"![ç”»åƒ {image_counter}]({local_path})")
                    print(f"Markdownãƒªãƒ³ã‚¯ç½®æ›: {image_url} -> ![ç”»åƒ {image_counter}]({local_path})")
                else:
                    # å˜ç´”ãªURLç½®æ›
                    updated_content = updated_content.replace(image_url, local_path)
                    print(f"ç”»åƒURLç½®æ›: {image_url} -> {local_path}")
                image_counter += 1
    
    if image_counter == 1:
        print("ç”»åƒURLã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
    else:
        print(f"åˆè¨ˆ {image_counter - 1} å€‹ã®ç”»åƒã‚’å‡¦ç†ã—ã¾ã—ãŸ")
    
    return updated_content

def create_diary_from_issue():
    """Issueã‹ã‚‰æ—¥è¨˜ã‚¨ãƒ³ãƒˆãƒªã‚’ä½œæˆ"""
    
    # ç’°å¢ƒå¤‰æ•°ã‹ã‚‰æƒ…å ±ã‚’å–å¾—
    issue_title = os.getenv('ISSUE_TITLE', '')
    issue_body = os.getenv('ISSUE_BODY', '')
    issue_number = os.getenv('ISSUE_NUMBER', '')
    issue_url = os.getenv('ISSUE_URL', '')
    issue_user = os.getenv('ISSUE_USER', 'Unknown')
    
    if not issue_body.strip():
        print("ã‚¨ãƒ©ãƒ¼: Issueå†…å®¹ãŒç©ºã§ã™")
        return
    
    # ç¾åœ¨ã®æ—¥æœ¬æ™‚é–“ã‚’å–å¾—
    jst_now = get_jst_now()
    date_str = jst_now.strftime('%Y-%m-%d')
    time_str = jst_now.strftime('%H:%M')
    
    print(f"Issue ã‹ã‚‰æ—¥è¨˜ä½œæˆ: {date_str} {time_str}")
    print(f"æŠ•ç¨¿è€…: {issue_user}")
    print(f"å†…å®¹: {issue_body[:100]}...")
    
    # ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
    diary_dir = Path('_diary')
    diary_dir.mkdir(exist_ok=True)
    
    file_path = diary_dir / f"{date_str}.md"
    
    # ç”»åƒå‡¦ç†ï¼šGitHubç”»åƒURLã‚’ãƒ­ãƒ¼ã‚«ãƒ«ã«ä¿å­˜
    processed_content = process_images_in_content(issue_body.strip(), date_str, time_str)
    
    # æ–°ã—ã„ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ï¼ˆMarkdownå½¢å¼ï¼‰
    new_message = f'''## {time_str}
{processed_content}'''
    
    if file_path.exists():
        # æ—¢å­˜ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ›´æ–°
        with open(file_path, 'r', encoding='utf-8') as f:
            content_lines = f.readlines()
        
        # Front matterã®çµ‚ã‚ã‚Šã‚’è¦‹ã¤ã‘ã‚‹
        front_matter_end = -1
        front_matter_count = 0
        for i, line in enumerate(content_lines):
            if line.strip() == '---':
                front_matter_count += 1
                if front_matter_count == 2:
                    front_matter_end = i
                    break
        
        if front_matter_end == -1:
            print("ã‚¨ãƒ©ãƒ¼: Front matterãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            return
        
        # æ–°ã—ã„ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¿½åŠ 
        content_lines.append('\n' + new_message + '\n')
        
        with open(file_path, 'w', encoding='utf-8') as f:
            f.writelines(content_lines)
        
        print(f"æ—¢å­˜ã®æ—¥è¨˜ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ›´æ–°ã—ã¾ã—ãŸ: {file_path}")
    
    else:
        # æ–°ã—ã„ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆ
        try:
            date_obj = datetime.strptime(date_str, '%Y-%m-%d')
            weekday = ['æœˆ', 'ç«', 'æ°´', 'æœ¨', 'é‡‘', 'åœŸ', 'æ—¥'][date_obj.weekday()]
            formatted_date = f"{date_obj.year}å¹´{date_obj.month:02d}æœˆ{date_obj.day:02d}æ—¥({weekday})"
        except:
            formatted_date = date_str
        
        front_matter = {
            'layout': 'diary',
            'title': f'{formatted_date}',
            'date': f'{date_str} 00:00:00 +0900'
        }
        
        content_text = f"""---
{yaml.dump(front_matter, default_flow_style=False, allow_unicode=True)}---

{new_message}
"""
        
        with open(file_path, 'w', encoding='utf-8') as f:
            f.write(content_text)
        
        print(f"æ–°ã—ã„æ—¥è¨˜ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆã—ã¾ã—ãŸ: {file_path}")

if __name__ == '__main__':
    create_diary_from_issue()
